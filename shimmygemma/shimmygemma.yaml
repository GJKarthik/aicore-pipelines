apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: shimmy-lightweight
  annotations:
    scenarios.ai.sap.com/description: "Lightweight Shimmy inference server with GPU support - models loaded at runtime"
    scenarios.ai.sap.com/name: "shimmy-lightweight"
    executables.ai.sap.com/description: "Shimmy inference service without baked models - fast deployment with GPU"
    executables.ai.sap.com/name: "shimmy-lightweight"
  labels:
    scenarios.ai.sap.com/id: "shimmy-lightweight"
    ai.sap.com/version: "2.0.0"
spec:
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: 1
        autoscaling.knative.dev/targetBurstCapacity: 0
      labels: |
        ai.sap.com/resourcePlan: infer.s
    spec: |
      predictor:
        imagePullSecrets:
        - name: ollamadocker
        minReplicas: 1
        maxReplicas: 3
        containers:
        - name: kserve-container
          image: docker.io/gjkarthik/shimmy@sha256:2361420ba9c5ca088d0d6a6871483008f83c4ff38393fa4ff5905785db238e63
          ports:
          - containerPort: 8080
            name: http1
            protocol: TCP
          env:
            - name: SHIMMY_PORT
              value: "8080"
            - name: SHIMMY_HOME
              value: "/opt/shimmy"
            - name: MODEL_PATH
              value: "/models"
          volumeMounts:
          - name: models
            mountPath: /models
          resources:
            requests:
              memory: "2Gi"
              cpu: "1000m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
