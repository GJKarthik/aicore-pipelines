apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: ollamainferm
  annotations:
    scenarios.ai.sap.com/description: "Run a ollama server on SAP AI Core with 32K context"
    scenarios.ai.sap.com/name: "ollamainferm"
    executables.ai.sap.com/description: "ollama service with 32K context window"
    executables.ai.sap.com/name: "ollamainferm"
  labels:
    scenarios.ai.sap.com/id: "ollamainferm"
    ai.sap.com/version: "0.0.3"
spec:
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: 1
        autoscaling.knative.dev/targetBurstCapacity: 0
      labels: |
        ai.sap.com/resourcePlan: infer.l
    spec: |
      predictor:
        imagePullSecrets:
        - name: ollamadocker
        minReplicas: 1
        maxReplicas: 1
        containers:
        - name: kserve-container
          image: docker.io/gjkarthik/ollama:ai-core
          ports:
            - containerPort: 8080
              protocol: TCP
          env:
            - name: OLLAMA_CONTEXT_LENGTH
              value: "32000"
            - name: OLLAMA_MAX_LOADED_MODELS
              value: "1"
            - name: OLLAMA_NUM_PARALLEL
              value: "1"
            - name: OLLAMA_FLASH_ATTENTION
              value: "1"
            - name: OLLAMA_KV_CACHE_TYPE
              value: "q8_0"
            - name: OLLAMA_GPU_LAYERS
              value: "-1"
            - name: OLLAMA_NUM_GPU
              value: "1"
            - name: OLLAMA_KEEP_ALIVE
              value: "24h"
          resources:
            requests:
              memory: "32Gi"
              cpu: "8"
              nvidia.com/gpu: "1"
            limits:
              memory: "48Gi"
              cpu: "12"
              nvidia.com/gpu: "1"
