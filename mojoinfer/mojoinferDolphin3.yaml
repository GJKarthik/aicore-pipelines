# SAP BTP AI Core Serving Template for nLocalModels
# Uses SAP Object Store artifact for model loading (recommended approach)
#
# Prerequisites:
#   1. Upload GGUF model to SAP AI Core artifact store
#   2. Create artifact reference pointing to the model
#   3. Docker registry secret (dockerhub-pull-secret)
#   4. HANA secret (nlocalmodels-hana-secret)
#
# Upload Model to AI Core Object Store:
#   1. Create artifact:
#      curl -X POST "https://<ai-core-url>/v2/lm/artifacts" \
#        -H "Authorization: Bearer $AI_CORE_TOKEN" \
#        -H "AI-Resource-Group: default" \
#        -H "Content-Type: application/json" \
#        -d '{
#          "name": "dolphin3-llama31-8b-q4km",
#          "kind": "model",
#          "url": "ai://default/dolphin3-llama31-8b-q4km",
#          "description": "Dolphin3.0-Llama3.1-8B Q4_K_M GGUF model"
#        }'
#
#   2. Upload model file to the artifact URL
#
apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: nlocalmodels-dolphin-artifact
  annotations:
    scenarios.ai.sap.com/description: "nLocalModels with Dolphin3.0-Llama3.1-8B from Object Store"
    scenarios.ai.sap.com/name: "nlocalmodels-inference"
    executables.ai.sap.com/description: "GGUF model loaded from SAP Object Store artifact"
    executables.ai.sap.com/name: "nlocalmodels-artifact-serve"
    artifacts.ai.sap.com/ggufmodel.kind: "model"
  labels:
    scenarios.ai.sap.com/id: "nlocalmodels-inference"
    ai.sap.com/version: "1.0.0"

spec:
  inputs:
    artifacts:
      - name: ggufmodel
        description: "GGUF model artifact from SAP Object Store"
    parameters:
      - name: model_name
        type: string
        default: "Dolphin3.0-Llama3.1-8B"
        description: "Display name for the model"
      - name: max_seq_len
        type: string
        default: "4096"
        description: "Maximum sequence length"
  
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: "2"
        autoscaling.knative.dev/targetBurstCapacity: "0"
        autoscaling.knative.dev/scaleToZeroPodRetentionPeriod: "15m"
      labels: |
        ai.sap.com/resourcePlan: infer.s
    spec: |
      predictor:
        imagePullSecrets:
          - name: dockerhub-pull-secret
        minReplicas: 1
        maxReplicas: 2
        containers:
          - name: kserve-container
            image: "docker.io/plturrell/sap-btp-images:ai-core-private-latest"
            ports:
              - containerPort: 8007
                protocol: TCP
            env:
              # Model loaded from artifact via STORAGE_URI
              - name: MODEL_NAME
                value: "{{inputs.parameters.model_name}}"
              - name: STORAGE_URI
                value: "{{inputs.artifacts.ggufmodel}}"
              - name: MODEL_PATH
                value: "/mnt/models"
              - name: LOCALMODEL_GGUF
                value: "/mnt/models/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf"
              
              # Serving engine
              - name: MAX_SEQ_LEN
                value: "{{inputs.parameters.max_seq_len}}"
              - name: MAX_BATCH_SIZE
                value: "64"
              - name: MAX_PREFILL_TOKENS
                value: "4096"
              - name: PREFILL_CHUNK_SIZE
                value: "512"
              
              # GPU configuration (T4)
              - name: ENABLE_GPU
                value: "true"
              - name: KERNEL_GPU_BACKEND
                value: "cuda"
              - name: NUM_GPU_BLOCKS
                value: "2048"
              - name: BLOCK_SIZE
                value: "16"
              
              # Memory
              - name: MAX_RAM_MB
                value: "14336"
              - name: KV_CACHE_RAM_MB
                value: "4096"
              - name: TENSOR_HOT_MB
                value: "6144"
              - name: TENSOR_WARM_MB
                value: "2048"
              
              # Features
              - name: ENABLE_PAGED_ATTENTION
                value: "true"
              - name: ENABLE_CHUNKED_PREFILL
                value: "true"
              - name: ENABLE_KV_TIERING
                value: "true"
              - name: ENABLE_TENSOR_TIERING
                value: "true"
              - name: ENABLE_SPECULATIVE
                value: "false"
              - name: NUM_DRAFT_TOKENS
                value: "5"
              - name: ENABLE_DISTRIBUTED
                value: "false"
              
              # Server
              - name: NLOCALMODELS_PORT
                value: "8007"
              - name: LOG_LEVEL
                value: "info"
              
              # HANA credentials from secret
              - name: SAP_HANA_ODATA_URL
                valueFrom:
                  secretKeyRef:
                    name: nlocalmodels-hana-secret
                    key: SAP_HANA_ODATA_URL
              - name: SAP_HANA_PORT
                valueFrom:
                  secretKeyRef:
                    name: nlocalmodels-hana-secret
                    key: SAP_HANA_PORT
              - name: SAP_HANA_USER
                valueFrom:
                  secretKeyRef:
                    name: nlocalmodels-hana-secret
                    key: SAP_HANA_USER
              - name: SAP_HANA_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: nlocalmodels-hana-secret
                    key: SAP_HANA_PASSWORD
              - name: SAP_HANA_SCHEMA
                valueFrom:
                  secretKeyRef:
                    name: nlocalmodels-hana-secret
                    key: SAP_HANA_SCHEMA
              - name: SAP_HANA_VECTOR_TABLE
                value: "MODEL_EMBEDDINGS"
              - name: NLOCALMODELS_ENABLE_HANA
                value: "true"
            
            resources:
              limits:
                nvidia.com/gpu: "1"
                memory: "16Gi"
                cpu: "4"
              requests:
                nvidia.com/gpu: "1"
                memory: "14Gi"
                cpu: "2"
            
            command: ["/app/ai-core-private-server"]
            args: 
              - "--port"
              - "8007"
            
            livenessProbe:
              httpGet:
                path: /health/live
                port: 8007
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 5
            
            readinessProbe:
              httpGet:
                path: /health/ready
                port: 8007
              initialDelaySeconds: 120
              periodSeconds: 15
              timeoutSeconds: 10
              failureThreshold: 3
            
            startupProbe:
              httpGet:
                path: /health/startup
                port: 8007
              initialDelaySeconds: 60
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 20
