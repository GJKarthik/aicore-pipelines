apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: vllm-local-model
  annotations:
    scenarios.ai.sap.com/description: "Run a vLLM server with a baked-in local model"
    scenarios.ai.sap.com/name: "vllm-local-model"
    executables.ai.sap.com/description: "Run a vLLM service for a local model"
    executables.ai.sap.com/name: "vllm-local-model"
  labels:
    scenarios.ai.sap.com/id: "vllm"
    ai.sap.com/version: "0.0.1"
spec:
  inputs:
    parameters:
      # --- MODIFIED PARAMETER for local model path ---
      - name: modelName 
        # Set default to the absolute path of the model directory *inside your Docker image*.
        # E.g., if your model is in /gemma-2b-it/ in the image, use that path.
        default: "/app/local_model/gemma-2-2b-it" 
        type: string 
        description: "The absolute file path to the model directory baked into the Docker image."
      # --- REQUIRED PARAMETER for GPU provisioning ---
      - name: resourcePlan
        default: "infer.m" # infer.m is a common GPU resource plan
        type: string 
        description: "The resource plan to use, e.g., 'infer.m' or 'infer.l' for GPU instances."
      # --- Existing vLLM Parameters ---
      - name: dataType
        default: "half" 
        type: string 
        description: "--dtype: Data type for model weights and activations."
      - name: gpuMemoryUtilization
        default: "0.9" 
        type: string 
        description: "The fraction (0~1) of GPU memory to be used by the model."
      - name: maxTokenLen
        default: "4096" 
        type: string
        description: "Maximum sequence length."
      - name: maxNumBatchedTokens
        default: "256000" 
        type: string
        description: "Maximum number of tokens per batch."
      - name: maxNumSeqs
        default: "256" 
        type: string
        description: "Maximum number of sequences per batch."
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: 1
        autoscaling.knative.dev/targetBurstCapacity: 0
      labels: |
        ai.sap.com/resourcePlan: "{{inputs.parameters.resourcePlan}}" # Ensure GPU Resource Plan is applied
    spec: |
      predictor:
        imagePullSecrets:
        - name: ollamadocker
        minReplicas: 1
        maxReplicas: 1
        containers:
        - name: kserve-container
          # --- MODIFIED: Use your custom image name ---
          image: docker.io/gjkarthik/vllm-gemma-2-2b-it:aicore
          ports:
            - containerPort: 8000
              protocol: TCP
          command: ["/bin/sh", "-c"]
          args:
            - >
              set -e && echo "-------------Starting vLLM OpenAI API Server--------------" 
              && python3 -m vllm.entrypoints.openai.api_server 
              --model {{inputs.parameters.modelName}} # Model path is now the local path from above
              --dtype {{inputs.parameters.dataType}}
              --gpu-memory-utilization {{inputs.parameters.gpuMemoryUtilization}}
              --enforce-eager
              --max-model-len {{inputs.parameters.maxTokenLen}}
              --max-num-batched-tokens {{inputs.parameters.maxNumBatchedTokens}}
              --max-num-seqs {{inputs.parameters.maxNumSeqs}}
